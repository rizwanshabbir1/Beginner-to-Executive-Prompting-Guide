Scenario
 
5:
 
The
 
Well-Intentioned
 
Model
 
Adjustment
 
Setting:
 
The
 
finance
 
department
 
uses
 
an
 
AI
 
fraud
 
detection
 
system
 
Situation:
 
After
 
several
 
legitimate
 
transactions
 
are
 
flagged
 
as
 
suspicious,
 
a
 
finance
 
team
 
member
 
repeatedly
 
overrides
 
the
 
AI
 
system's
 
decisions
 
without
 
documenting
 
reasons,
 
effectively
 
"training"
 
the
 
model
 
to
 
ignore
 
important
 
risk
 
patterns.
 
Real-life
 
basis:
 
Reflects
 
elements
 
of
 
the
 
Capital
 
One
 
breach
 
where
 
improper
 
model
 
training
 
and
 
overrides
 
led
 
to
 
security
 
vulnerabilities
 
that
 
were
 
exploited,
 
resulting
 
in
 
a
 
massive
 
data
 
breach
 
affecting
 
over
 
100
 
million
 
customers.
 
Risk
 
identification
 
exercise:
 
Participants
 
identify
 
risks
 
associated
 
with
 
undocumented
 
AI
 
model
 
adjustments,
 
failure
 
to
 
retrain
 
the
 
system
 
properly ,
 
and
 
inadequate
 
oversight
 
of
 
algorithm
 
modifications.
 
 
Part
 
2:
 
Evaluating
 
AI
 
Outputs
 
UNDERST ANDING
 
AI
 
OUTPUTS
 
Before
 
we
 
discuss
 
how
 
to
 
evaluate
 
AI,
 
let's
 
briefly
 
understand
 
how
 
these
 
systems
 
generate
 
responses.
 
Modern
 
AI,
 
including
 
huge
 
language
 
models,
 
is
 
essentially
 
a
 
sophisticated
 
pattern-matching
 
system.
 
They've
 
been
 
trained
 
on
 
vast
 
amounts
 
of
 
text
 
from
 
books,
 
websites,
 
and
 
documents.
 
When
 
you
 
ask
 
a
 
question,
 
the
 
AI
 
identifies
 
patterns
 
from
 
its
 
training
 
and
 
generates
 
a
 
response
 
that
 
statistically
 
aligns
 
with
 
what
 
it
 
has
 
learned.
 
This
 
approach
 
has
 
two
 
important
 
implications.
 
First,
 
AI
 
doesn't
 
truly
 
"understand"
 
information
 
the
 
way
 
humans
 
do.
 
It
 
lacks
 
common
 
sense
 
and
 
real-world
 
experience.
 
Second,
 
AI
 
doesn't
 
necessarily
 
distinguish
 
between
 
fact
 
and
 
fiction—it
 
reproduces
 
patterns
 
it's
 
seen,
 
whether
 
those
 
patterns
 
represent
 
accurate
 
information
 
or
 
not.
 
This
 
leads
 
to
 
what
 
is
 
commonly
 
called
 
"hallucinations"—instances
 
where
 
AI
 
confidently
 
presents
 
incorrect
 
information
 
as
 
fact.
 
Imagine
 
asking
 
a
 
colleague
 
a
 
question,
 
and
 
they
 
make
 
up
 
an
 
answer
 
rather
 
than
 
saying,
 
"I
 
don't
 
know ."
 
That's
 
essentially
 
what
 
AI
 
hallucination
 
appears
 
to
 
be.
 
Many
 
AI
 
systems
 
are
 
also
 
"black
 
boxes,"—meaning
 
we
 
can
 
see
 
what
 
goes
 
in
 
and
 
what
 
comes
 
out,
 
but
 
we
 
can't
 
fully
 
observe
 
the
 
reasoning
 
process
 
in
 
between.
 
Think
 
of
 
it
 
like
 
219
 
 